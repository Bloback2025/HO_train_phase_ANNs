import json, argparse
from pathlib import Path
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.metrics import mean_absolute_error, r2_score

def ensure_lags(df: pd.DataFrame, base_cols, max_lag: int):
    for col in base_cols:
        if col not in df.columns:
            raise ValueError(f"Missing base column: {col}")
        for k in range(1, max_lag + 1):
            lag_col = f"{col}_lag{k}"
            if lag_col not in df.columns:
                df[lag_col] = df[col].shift(k)
    return df

def ensure_target(df: pd.DataFrame, base_close="Close", target_col="Close_t+1"):
    if target_col not in df.columns:
        if base_close not in df.columns:
            raise ValueError(f"Missing base column for target: {base_close}")
        df[target_col] = df[base_close].shift(-1)
    return df

def chronological_split(X, y, test_frac=0.15):
    n = len(X)
    test_n = int(round(test_frac * n))
    if test_n <= 0:
        return X, pd.DataFrame(columns=X.columns), y, pd.Series(dtype=float)
    return X.iloc[:-test_n], X.iloc[-test_n:], y.iloc[:-test_n], y.iloc[-test_n:]

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--manifest", required=True)
    ap.add_argument("--csv", default="./hoxnc_full.csv")
    ap.add_argument("--model", default="./models/best.keras")
    ap.add_argument("--test_frac", type=float, default=0.15)
    args = ap.parse_args()

    # Deterministic run log markers
    print("RUN_LOG: start", Path(args.csv).name); 
    try:
        man = json.loads(Path(args.manifest).read_text(encoding="utf-8"))
    except Exception as e:
        print("RUN_ERROR: manifest_load_failed", str(e)); raise

    features = man.get("features", [])
    target = man.get("target", "Close_t+1")

    base_cols = ["Open", "High", "Low", "Close"]
    max_lag = 0
    if any("_lag" in f for f in features):
        try:
            max_lag = max(int(f.split("_lag")[-1]) for f in features if "_lag" in f)
        except Exception:
            max_lag = 0

    df = pd.read_csv(args.csv)
    ### FEATURES_FALLBACK_AND_SELECTIVE_DROPNA_v1 ###
    # If manifest.features is empty, derive conservative feature list from CSV header (exclude Date and target)
    try:
        if not locals().get('features'):
            _csv_path_for_infer = getattr(args, "csv", "./hoxnc_full.csv")
            try:
                _imported_df_head = pd.read_csv(_csv_path_for_infer, nrows=1)
                csv_cols = list(_imported_df_head.columns)
            except Exception:
                csv_cols = []
            # decide target name
            _target_name = target if target else "Close_t+1"
            # conservative features: all CSV columns except Date and target
            features = [c for c in csv_cols if c not in ("Date", _target_name)]
            # if still empty, fallback to base_cols with lags up to max_lag if present
            if not features:
                features = []
                for base in ["Open","High","Low","Close"]:
                    for k in range(1, max(1, max_lag)+1):
                        features.append(f"{base}_lag{k}")
                # if that also yields empty, at least include Close for sanity
                if not features:
                    if "Close" in csv_cols:
                        features = ["Close"]
    except Exception as __e:
        print("RUN_ERROR: feature_fallback_failed", str(__e)); raise

    # Replace global dropna() with selective drop on features + target to avoid removing rows unnecessarily
    try:
        _required_cols = list(features) + ([target] if target else [])
        # keep only rows that have at least required columns present (non-null)
        df = df.dropna(axis=0, subset=_required_cols).reset_index(drop=True)
    except Exception as __e:
        print("RUN_ERROR: selective_dropna_failed", str(__e)); raise
    ### END FEATURES_FALLBACK_AND_SELECTIVE_DROPNA_v1 ###

    # Build engineered inputs
    df = ensure_lags(df, base_cols, max_lag)
    df = ensure_target(df, base_close="Close", target_col=target)

    # Drop NaNs from shifting and align
    df = df.dropna().reset_index(drop=True)

    # Select features exactly in manifest order
    missing = [c for c in features if c not in df.columns]
    if missing:
        print("RUN_ERROR: missing_after_lag_construction", missing)
        raise ValueError(f"Missing columns after lag construction: {missing}")

    X = df[features].astype(float)
    y = df[target].astype(float)

    X_train, X_test, y_train, y_test = chronological_split(X, y, test_frac=args.test_frac)

    model = tf.keras.models.load_model(args.model)
    y_pred = model.predict(X_test, verbose=0).reshape(-1)

    # Metrics
    mae = mean_absolute_error(y_test, y_pred) if len(y_test) > 0 else float("nan")
    r2  = r2_score(y_test, y_pred) if len(y_test) > 0 else float("nan")
    print(f"[METRICS] test_mae={mae:.6f} r2_ann={r2:.6f}")

    # Final marker
    print("RUN_COMPLETE: SUCCESS")

if __name__ == "__main__":
    main()




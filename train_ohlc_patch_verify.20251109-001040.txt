VERIFICATION: train_ohlc.py lines 1..120 of 120 total
================================================================================
   1: import json, argparse, time, hashlib
   2: from pathlib import Path
   3: import pandas as pd
   4: import numpy as np
   5: import tensorflow as tf
   6: from sklearn.preprocessing import StandardScaler
   7: from sklearn.metrics import mean_absolute_error, r2_score
   8: 
   9: SEED = 42
  10: tf.random.set_seed(SEED)
  11: np.random.seed(SEED)
  12: 
  13: def chronological_split_idx(n, test_frac=0.15, val_frac=0.1):
  14:     test_n = int(round(test_frac * n))
  15:     val_n = int(round(val_frac * (n - test_n)))
  16:     train_end = n - test_n - val_n
  17:     return train_end, train_end + val_n
  18: 
  19: def build_model(input_dim):
  20:     inp = tf.keras.Input(shape=(input_dim,), name="ohlc_input")
  21:     x = tf.keras.layers.Dense(32, activation="relu")(inp)
  22:     x = tf.keras.layers.Dense(16, activation="relu")(x)
  23:     out = tf.keras.layers.Dense(1, activation="linear")(x)
  24:     m = tf.keras.Model(inputs=inp, outputs=out)
  25:     m.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
  26:               loss="mse")
  27:     return m
  28: 
  29: def sha256_of_file(p):
  30:     try:
  31:         b = Path(p).read_bytes()
  32:         import hashlib
  33:         return hashlib.sha256(b).hexdigest()
  34:     except Exception:
  35:         return None
  36: 
  37: def main():
  38:     ap = argparse.ArgumentParser()
  39:     ap.add_argument("--csv", default="./hoxnc_full.csv")
  40:     ap.add_argument("--manifest", default="./hoxnc_training.with_base_and_lags.manifest.json")
  41:     ap.add_argument("--out_model", default="./models/ohlc_best.keras")
  42:     ap.add_argument("--epochs", type=int, default=50)
  43:     ap.add_argument("--batch", type=int, default=32)
  44:     args = ap.parse_args()
  45: 
  46:     man = json.loads(Path(args.manifest).read_text(encoding="utf-8"))
  47:     features = man.get("features", ["Open","High","Low","Close"])
  48:     target = man.get("target", "Close_t+1")
  49: 
  50:     df = pd.read_csv(args.csv)
  51:     if target not in df.columns and "Close" in df.columns:
  52:         df[target] = df["Close"].shift(-1)
  53:     req = list(features) + [target]
  54:     df = df.dropna(axis=0, subset=req).reset_index(drop=True)
  55: 
  56:     X = df[features].astype(float).values
  57:     y = df[target].astype(float).values.reshape(-1,1)
  58: 
  59:     n = len(X)
  60:     if n < 10:
  61:         raise SystemExit("NOT_ENOUGH_ROWS")
  62: 
  63:     train_end, val_end = chronological_split_idx(n, test_frac=0.15, val_frac=0.1)
  64:     X_train = X[:train_end]
  65:     X_val = X[train_end:val_end]
  66:     X_test = X[val_end:]
  67:     y_train = y[:train_end]
  68:     y_val = y[train_end:val_end]
  69:     y_test = y[val_end:]
  70: 
  71:     scaler = StandardScaler()
  72:     X_train_s = scaler.fit_transform(X_train)
  73:     X_val_s = scaler.transform(X_val)
  74:     X_test_s = scaler.transform(X_test)
  75: 
  76:     model = build_model(input_dim=X_train_s.shape[1])
  77: 
  78:     outdir = Path(args.out_model)
  79:     outdir.parent.mkdir(parents=True, exist_ok=True)
  80:     ckpt = tf.keras.callbacks.ModelCheckpoint(filepath=str(outdir), monitor="val_loss", save_best_only=True)
  81:     es = tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=8, restore_best_weights=True)
  82: 
  83:     history = model.fit(X_train_s, y_train, validation_data=(X_val_s, y_val),
  84:                         epochs=args.epochs, batch_size=args.batch, callbacks=[ckpt, es], verbose=2)
  85: 
  86:     try:
  87:         model = tf.keras.models.load_model(str(outdir))
  88:     except Exception:
  89:         pass
  90: 
  91:     y_pred = model.predict(X_test_s).reshape(-1)
  92:     mae = mean_absolute_error(y_test, y_pred)
  93:     r2 = r2_score(y_test, y_pred)
  94: 
  95:     tm = {
  96:         "timestamp": time.strftime("%Y-%m-%dT%H:%M:%S"),
  97:         "train_rows": int(len(X_train)),
  98:         "val_rows": int(len(X_val)),
  99:         "test_rows": int(len(X_test)),
 100:         "manifest_used": str(Path(args.manifest).resolve()),
 101:         "manifest_sha256": sha256_of_file(str(Path(args.manifest).resolve())),
 102:         "csv_used": str(Path(args.csv).resolve()),
 103:         "csv_sha256": sha256_of_file(str(Path(args.csv).resolve())),
 104:         "model_path": str(Path(args.out_model).resolve()),
 105:         "model_input_dim": int(X_train_s.shape[1]),
 106:         "epochs_ran": len(history.history.get("loss", [])),
 107:         "test_mae": float(mae),
 108:         "test_r2": float(r2)
 109:     }
 110:     pm_path = Path("training_manifests")
 111:     pm_path.mkdir(exist_ok=True)
 112:     p = pm_path.joinpath("training_manifest." + time.strftime("%Y%m%d-%H%M%S") + ".json")
 113:     p.write_text(json.dumps(tm, indent=2), encoding="utf-8")
 114: 
 115:     print("TRAINING_COMPLETE: model_saved=" + str(Path(args.out_model).resolve()))
 116:     print(f"[METRICS] test_mae={mae:.6f} test_r2={r2:.6f}")
 117: 
 118: if __name__ == "__main__":
 119:     main()
 120: 


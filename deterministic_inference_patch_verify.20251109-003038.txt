    if any("_lag" in f for f in features):
        try:
            max_lag = max(int(f.split("_lag")[-1]) for f in features if "_lag" in f)
        except Exception:
            max_lag = 0

    df = pd.read_csv(args.csv)
    ### FEATURES_FALLBACK_AND_SELECTIVE_DROPNA_v1 ###
    # If manifest.features is empty, derive conservative feature list from CSV header (exclude Date and target)
    try:
        if not locals().get('features'):
            _csv_path_for_infer = getattr(args, "csv", "./hoxnc_full.csv")
            try:
                _imported_df_head = pd.read_csv(_csv_path_for_infer, nrows=1)
                csv_cols = list(_imported_df_head.columns)
            except Exception:
                csv_cols = []
            # decide target name
            _target_name = target if target else "Close_t+1"
            # conservative features: all CSV columns except Date and target
            features = [c for c in csv_cols if c not in ("Date", _target_name)]
            # if still empty, fallback to base_cols with lags up to max_lag if present
            if not features:
                features = []
                for base in ["Open","High","Low","Close"]:
                    for k in range(1, max(1, max_lag)+1):
                        features.append(f"{base}_lag{k}")
                # if that also yields empty, at least include Close for sanity
                if not features:
                    if "Close" in csv_cols:
                        features = ["Close"]
    except Exception as __e:
        print("RUN_ERROR: feature_fallback_failed", str(__e)); raise

    # Replace global dropna() with selective drop on features + target to avoid removing rows unnecessarily
    try:
        _required_cols = list(features) + ([target] if target else [])
        # keep only rows that have at least required columns present (non-null)
        # --- BEGIN: safe target creation, selective dropna, and scaler load ---
import os, pickle
from sklearn.preprocessing import StandardScaler
# create target if missing but Close present
if ("Close_t+1" not in df.columns) and ("Close" in df.columns):
    try:
        df["Close_t+1"] = df["Close"].shift(-1)
        # drop the final NaN row created by the shift to keep lengths consistent
        df = df.dropna(subset=["Close_t+1"]).reset_index(drop=True)
        print("RUN_INFO: created Close_t+1 from Close; rows=", len(df))
    except Exception as __e:
        print("RUN_WARN: target_creation_failed", str(__e))
# build required cols only including target if present
_required_cols = list(features) + ([target] if (target in df.columns) else [])
# perform selective dropna safely
try:
    df = df.dropna(axis=0, subset=_required_cols).reset_index(drop=True)
except Exception as __e:
    print("RUN_ERROR: selective_dropna_failed", str(__e))
    raise
# Attempt to load persisted scaler for consistent preprocessing
scaler_path = os.path.join("models", "scaler.pkl")
scaler = None
if os.path.exists(scaler_path):
    try:
        with open(scaler_path, "rb") as __sf:
            scaler = pickle.load(__sf)
        print("RUN_INFO: loaded scaler from", scaler_path)
    except Exception as __e:
        print("RUN_WARN: failed_loading_scaler", str(__e))
# If scaler not loaded, fit on training slice (mirror training split)
if scaler is None:
    try:
        n = len(df)
        test_n = int(round(0.15 * n))
        val_n = int(round(0.1 * (n - test_n)))
        train_end = n - test_n - val_n
        X_train = df[features].astype(float).values[:train_end]
        scaler = StandardScaler().fit(X_train)
        print("RUN_INFO: fitted fallback scaler on training slice")
    except Exception as __e:
        print("RUN_WARN: scaler_fallback_failed", str(__e))
# --- END: safe preprocessing block ---
    except Exception as __e:
        print("RUN_ERROR: selective_dropna_failed", str(__e)); raise
    ### END FEATURES_FALLBACK_AND_SELECTIVE_DROPNA_v1 ###

    # Build engineered inputs
    df = ensure_lags(df, base_cols, max_lag)
    df = ensure_target(df, base_close="Close", target_col=target)

    # Drop NaNs from shifting and align
    df = df.dropna().reset_index(drop=True)

    # Select features exactly in manifest order
    missing = [c for c in features if c not in df.columns]
    if missing:
        print("RUN_ERROR: missing_after_lag_construction", missing)
        raise ValueError(f"Missing columns after lag construction: {missing}")

    X = df[features].astype(float)
    y = df[target].astype(float)

    X_train, X_test, y_train, y_test = chronological_split(X, y, test_frac=args.test_frac)

    model = tf.keras.models.load_model(args.model)
    y_pred = model.predict(X_test, verbose=0).reshape(-1)

    # Metrics
    mae = mean_absolute_error(y_test, y_pred) if len(y_test) > 0 else float("nan")
    r2  = r2_score(y_test, y_pred) if len(y_test) > 0 else float("nan")
    print(f"[METRICS] test_mae={mae:.6f} r2_ann={r2:.6f}")

    # Final marker
    print("RUN_COMPLETE: SUCCESS")

if __name__ == "__main__":
    main()




